{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afp9JU9r0aCe"
      },
      "source": [
        "#DOWNLOAD DEPENDENCIES\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-13RPvBe5VJ2",
        "outputId": "354b6312-9189-40c2-e2d8-7b2fc132b351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/mohak/.zshenv:1: bad assignment\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "/Users/mohak/.zshenv:1: bad assignment\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "/Users/mohak/.zshenv:1: bad assignment\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "/Users/mohak/.zshenv:1: bad assignment\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# CELL 1: Install Dependencies\n",
        "!pip install -q transformers torch accelerate bitsandbytes\n",
        "!pip install -q networkx matplotlib pyvis\n",
        "!pip install -q sentencepiece protobuf\n",
        "!pip install -q compressed-tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_qjl1CU09xx",
        "outputId": "07dc4e9a-dedc-4365-b6a8-336621e81627"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mohak/Desktop/Lab Work/labenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All imports successful\n",
            "✓ CUDA available: False\n",
            "✓ Device: CPU\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: Validate Imports\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvis.network import Network\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All imports successful\")\n",
        "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"✓ Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lJJ9KEdA2GZ2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "root = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSTJhb360ezU"
      },
      "source": [
        "# BIOMISTRAL SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGZkvILCKCGB",
        "outputId": "c2abfa64-c163-4135-e7ad-4f2ec876e2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded: biomistral/BioMistral-7B\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'MAIN_DATA.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-259890888.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'MAIN_DATA.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mpapers_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MAIN_DATA.json'"
          ]
        }
      ],
      "source": [
        "# CELL: Matching Diagnostic Test Format\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Model setup - SWITCHED TO BIOMISTRAL\n",
        "model_name = \"biomistral/BioMistral-7B\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(f\"✓ Model loaded: {model_name}\")\n",
        "\n",
        "# Load data\n",
        "with open('MAIN_DATA.json', 'r') as f:\n",
        "    papers_data = json.load(f)\n",
        "\n",
        "def prepare_papers(papers_data):\n",
        "    papers = []\n",
        "    for paper_id, paper_info in papers_data.items():\n",
        "        title = paper_info.get('name', 'Untitled')\n",
        "        chunks = paper_info.get('chunks', [])\n",
        "        full_text = ' '.join(chunks)\n",
        "        papers.append((paper_id, title, full_text))\n",
        "    return papers\n",
        "\n",
        "papers_list = prepare_papers(papers_data)\n",
        "print(f\"✓ Loaded {len(papers_list)} papers\")\n",
        "\n",
        "# FIXED inference - no system role, just user message\n",
        "def extract_relationships_batch(papers_list, max_papers=None):\n",
        "    if max_papers:\n",
        "        papers_list = papers_list[:max_papers]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for idx, (paper_id, title, paper_text) in enumerate(papers_list):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing {idx+1}/{len(papers_list)} - Paper ID: {paper_id}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        try:\n",
        "            # Truncate paper\n",
        "            truncated_text = paper_text[:6000]\n",
        "            \n",
        "            # Build complete user content (system instructions + task)\n",
        "            user_content = f\"{EXTRACTION_TASK}{truncated_text}\\n\\nJSON output:\"\n",
        "            \n",
        "            # FIXED: Only user role, no system role\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": user_content\n",
        "                }\n",
        "            ]\n",
        "            \n",
        "            # Apply chat template\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                tokenize=True,\n",
        "                return_dict=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(model.device)\n",
        "            \n",
        "            input_len = inputs[\"input_ids\"].shape[-1]\n",
        "            print(f\"Input tokens: {input_len}\")\n",
        "            \n",
        "            # Generate\n",
        "            with torch.inference_mode():\n",
        "                generation = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=512,\n",
        "                    do_sample=False\n",
        "                )\n",
        "            \n",
        "            # Extract only generated tokens\n",
        "            generation = generation[0][input_len:]\n",
        "            decoded = tokenizer.decode(generation, skip_special_tokens=True)\n",
        "            \n",
        "            result = {\n",
        "                'paper_id': paper_id,\n",
        "                'paper_title': title,\n",
        "                'raw_output': decoded,\n",
        "                'success': True\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            print(f\"✓ Generated {len(decoded)} chars\")\n",
        "            print(f\"Preview: {decoded[:300]}...\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ ERROR: {str(e)}\")\n",
        "            results.append({\n",
        "                'paper_id': paper_id,\n",
        "                'raw_output': None,\n",
        "                'error': str(e),\n",
        "                'success': False\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Test\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TESTING WITH 3 PAPERS\")\n",
        "print(\"=\"*60)\n",
        "raw_results = extract_relationships_batch(papers_list, max_papers=3)\n",
        "\n",
        "# Save results to file\n",
        "output_file = 'extraction_data_biomistral.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(raw_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Results saved to {output_file}\")\n",
        "\n",
        "# Show results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS:\")\n",
        "print(\"=\"*60)\n",
        "for i, result in enumerate(raw_results):\n",
        "    print(f\"\\n--- Paper {i+1}: {result['paper_id']} ---\")\n",
        "    print(f\"Success: {result['success']}\")\n",
        "    if result['success']:\n",
        "        print(f\"Output length: {len(result['raw_output'])}\")\n",
        "        print(f\"Output:\\n{result['raw_output'][:500]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "798gB2ab3Uaz"
      },
      "source": [
        "===========================================================================================================\n",
        "#KIMI K2 TURBO VIA LAVA ROUTER\n",
        "==========================================================================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zP3E0DZ3E_m",
        "outputId": "fae31993-5384-4f97-d863-dd1f27379456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Data type: <class 'dict'>\n",
            "Keys: ['1', '2', '3', '4', '5']\n",
            "Total papers available: 2026\n",
            "\n",
            "Processing 3 papers...\n",
            "\n",
            "Processing paper 1/3...\n",
            "  Status: Success\n",
            "  Tokens: {'prompt_tokens': 14367, 'completion_tokens': 213, 'total_tokens': 14580, 'cached_tokens': 14367}\n",
            "\n",
            "Processing paper 2/3...\n",
            "  Status: Success\n",
            "  Tokens: {'prompt_tokens': 3495, 'completion_tokens': 41, 'total_tokens': 3536, 'cached_tokens': 3495}\n",
            "\n",
            "Processing paper 3/3...\n",
            "  Status: Success\n",
            "  Tokens: {'prompt_tokens': 351, 'completion_tokens': 78, 'total_tokens': 429, 'cached_tokens': 351}\n",
            "\n",
            "Saving results to extraction_results.json...\n",
            "\n",
            "==================================================\n",
            "SUMMARY\n",
            "==================================================\n",
            "Total papers processed: 3\n",
            "Successful: 3\n",
            "Failed: 0\n",
            "Results saved to: extraction_results.json\n",
            "\n",
            "==================================================\n",
            "SAMPLE EXTRACTION (Paper 1)\n",
            "==================================================\n",
            "{\"disease\": \"oral squamous cell carcinoma\", \"bacteria_relationships\": [{\"bacteria\": \"Fusobacterium nucleatum\", \"direction\": \"increased\"}, {\"bacteria\": \"Prevotella intermedia\", \"direction\": \"increased\"}, {\"bacteria\": \"Aggregatibacter segnis\", \"direction\": \"increased\"}, {\"bacteria\": \"Capnocytophaga leadbetteri\", \"direction\": \"increased\"}, {\"bacteria\": \"Peptostreptococcus stomatis\", \"direction\": \"increased\"}, {\"bacteria\": \"Catonella morbi\", \"direction\": \"increased\"}, {\"bacteria\": \"Gemella morbillorum\", \"direction\": \"increased\"}, {\"bacteria\": \"Campylobacter rectus\", \"direction\": \"increased\"}, {\"bacteria\": \"Streptococcus oralis\", \"direction\": \"decreased\"}]}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import base64\n",
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "LAVA_API_KEY = userdata.get('LAVA_API_KEY')\n",
        "CONNECTION_SECRET = userdata.get('CONNECTION_SECRET')\n",
        "PRODUCT_SECRET = userdata.get('PRODUCT_SECRET')\n",
        "\n",
        "LAVA_FORWARD_URL = \"https://api.lavapayments.com/v1/forward\"\n",
        "PROVIDER_API_URL = \"https://api.moonshot.ai/v1/chat/completions\"\n",
        "MODEL_NAME = \"kimi-k2-turbo-preview\"\n",
        "\n",
        "INPUT_FILE = root + \"MAIN_DATA.json\"  # Assumes file is in current directory\n",
        "OUTPUT_FILE = \"extraction_results.json\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "def call_llm(user_message, max_tokens=2048, temperature=0.3):\n",
        "    lava_url = f\"{LAVA_FORWARD_URL}/{PROVIDER_API_URL}\"\n",
        "\n",
        "    lava_token = {\"secret_key\": LAVA_API_KEY}\n",
        "    if CONNECTION_SECRET:\n",
        "        lava_token[\"connection_secret\"] = CONNECTION_SECRET\n",
        "    if PRODUCT_SECRET:\n",
        "        lava_token[\"product_secret\"] = PRODUCT_SECRET\n",
        "\n",
        "    encoded_token = base64.b64encode(\n",
        "        json.dumps(lava_token).encode('utf-8')\n",
        "    ).decode('utf-8')\n",
        "\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {encoded_token}',\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": user_message}]\n",
        "    }\n",
        "\n",
        "    response = requests.post(lava_url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def extract_relationships(paper_text, paper_id):\n",
        "    prompt = EXTRACTION_TASK + paper_text\n",
        "\n",
        "    try:\n",
        "        response = call_llm(prompt, max_tokens=4096, temperature=0.3)\n",
        "        result = response['choices'][0]['message']['content']\n",
        "        return {\n",
        "            \"paper_id\": paper_id,\n",
        "            \"success\": True,\n",
        "            \"extraction\": result,\n",
        "            \"tokens_used\": response.get('usage', {})\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"paper_id\": paper_id,\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# =============================================================================\n",
        "# PROCESS FILES\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading data...\")\n",
        "with open(INPUT_FILE, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Debug: Check data structure\n",
        "print(f\"Data type: {type(data)}\")\n",
        "if isinstance(data, dict):\n",
        "    print(f\"Keys: {list(data.keys())[:5]}\")\n",
        "    # Convert dict to list of papers\n",
        "    papers_list = list(data.values())\n",
        "elif isinstance(data, list):\n",
        "    papers_list = data\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected data type: {type(data)}\")\n",
        "\n",
        "print(f\"Total papers available: {len(papers_list)}\")\n",
        "\n",
        "# Process first 3 papers as test\n",
        "papers_to_process = papers_list[:3]\n",
        "results = []\n",
        "\n",
        "print(f\"\\nProcessing {len(papers_to_process)} papers...\\n\")\n",
        "\n",
        "for i, paper in enumerate(papers_to_process, 1):\n",
        "    print(f\"Processing paper {i}/{len(papers_to_process)}...\")\n",
        "\n",
        "    # Get paper text (adjust based on your data structure)\n",
        "    if 'chunks' in paper:\n",
        "        paper_text = ' '.join(paper['chunks'])\n",
        "    elif 'text' in paper:\n",
        "        paper_text = paper['text']\n",
        "    else:\n",
        "        paper_text = str(paper)\n",
        "\n",
        "    paper_id = paper.get('id', f'paper_{i}')\n",
        "\n",
        "    # Extract relationships\n",
        "    result = extract_relationships(paper_text, paper_id)\n",
        "    results.append(result)\n",
        "\n",
        "    print(f\"  Status: {'Success' if result['success'] else 'Failed'}\")\n",
        "    if result['success']:\n",
        "        print(f\"  Tokens: {result['tokens_used']}\")\n",
        "    print()\n",
        "\n",
        "    # Rate limiting\n",
        "    if i < len(papers_to_process):\n",
        "        time.sleep(1)\n",
        "\n",
        "# Save results\n",
        "print(f\"Saving results to {OUTPUT_FILE}...\")\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    json.dump(results, indent=2, fp=f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total papers processed: {len(results)}\")\n",
        "print(f\"Successful: {sum(1 for r in results if r['success'])}\")\n",
        "print(f\"Failed: {sum(1 for r in results if not r['success'])}\")\n",
        "print(f\"Results saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "# Display first result\n",
        "if results and results[0]['success']:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SAMPLE EXTRACTION (Paper 1)\")\n",
        "    print(\"=\"*50)\n",
        "    print(results[0]['extraction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-ZzXt9aG33X"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
